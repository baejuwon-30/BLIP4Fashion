{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **1. 데이터셋 준비**"],"metadata":{"id":"ugkGLOFaBUPX"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import transforms\n","\n","import os\n","import json\n","import zipfile\n","import random\n","import numpy as np\n","from PIL import Image"],"metadata":{"id":"wfq5EDZcFiSJ","executionInfo":{"status":"ok","timestamp":1723635948752,"user_tz":-540,"elapsed":4935,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **1-1. 데이터셋 불러오기**"],"metadata":{"id":"evJKNiT-wxka"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8sBIzzJrGRVN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723635951575,"user_tz":-540,"elapsed":2825,"user":{"displayName":"차수빈","userId":"16979887891359732529"}},"outputId":"b6b75b96-32c3-479c-d917-a35daa4201ee"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**이미지 파일 압축 해제**\n","- 본인 경로를 제대로 설정했는지 확인\n","- 세션 디스크에 `data > train` 폴더를 만들어두고 압축 해제 진행하셔야 합니다..!\n","  - 세션 디스크 내 임시 경로로, 드라이브 x\n","\n","```\n","!unzip (데이터 경로) -d (압축 해제할 경로)\n","```"],"metadata":{"id":"pq8MSIsKw5W0"}},{"cell_type":"code","source":["### 이미지 파일 압축 해제\n","\n","!unzip /content/drive/MyDrive/Euron/6th-project/final/data/train.zip -d /content/data/train\n","!unzip /content/drive/MyDrive/Euron/6th-project/final/data/val.zip -d /content/data/val"],"metadata":{"id":"ti20ZaV5GriF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### annotations 파일 준비\n","\n","coco_train_path = '/content/drive/MyDrive/Euron/6th-project/final/data/coco_train.json'\n","coco_val_path = '/content/drive/MyDrive/Euron/6th-project/final/data/coco_val.json'"],"metadata":{"id":"g0Ly5dbrHIhI","executionInfo":{"status":"ok","timestamp":1723635951575,"user_tz":-540,"elapsed":3,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## **1-2. Custom Dataset 준비**"],"metadata":{"id":"dAm9-PxsxZUP"}},{"cell_type":"code","source":["### Custom Dataset 클래스 정의\n","\n","class FashionDataset(Dataset):\n","  def __init__(self, annotation_file, image_dir, transform = None):\n","    with open(annotation_file, 'r') as f:\n","      self.coco_data = json.load(f)\n","    self.image_dir = image_dir\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.coco_data['annotations'])\n","\n","  def __getitem__(self, idx):\n","    ## 속성 정보\n","    annotation = self.coco_data['annotations'][idx]\n","\n","    ## 이미지\n","    image_id = annotation['image_id']\n","\n","    image_info = next((img for img in self.coco_data['images'] if img['id'] == image_id), None)\n","    image_path = os.path.join(self.image_dir, image_info['file_name'])\n","\n","    image = Image.open(image_path).convert('RGB')\n","    if self.transform:\n","      image = self.transform(image)\n","\n","    ## 캡션\n","    caption = annotation['caption']\n","\n","    return image, caption"],"metadata":{"id":"5_LXU47iFnCj","executionInfo":{"status":"ok","timestamp":1723635951575,"user_tz":-540,"elapsed":2,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["## 데이터셋 준비\n","\n","# 이미지 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((384, 384)),\n","    transforms.ToTensor(),\n","])"],"metadata":{"id":"RPRv9YFtGMsh","executionInfo":{"status":"ok","timestamp":1723635952282,"user_tz":-540,"elapsed":4,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_dataset = FashionDataset(annotation_file = coco_train_path,\n","                               image_dir = '/content/data/train',\n","                               transform = transform)"],"metadata":{"id":"DFWH1ipf0HKT","executionInfo":{"status":"ok","timestamp":1723635953765,"user_tz":-540,"elapsed":5,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["val_dataset = FashionDataset(annotation_file = coco_val_path,\n","                               image_dir = '/content/data/val',\n","                               transform = transform)"],"metadata":{"id":"Kshpd3d50Hha","executionInfo":{"status":"ok","timestamp":1723635955225,"user_tz":-540,"elapsed":769,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["### DataLoader 설정\n","\n","train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n","val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = False)"],"metadata":{"id":"9DezaQbvHL0T","executionInfo":{"status":"ok","timestamp":1723635955225,"user_tz":-540,"elapsed":1,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# **2. Training(fine-tuning)**"],"metadata":{"id":"YOiy3v8wHPfL"}},{"cell_type":"code","source":["!pip install peft"],"metadata":{"id":"M5FquHmV8Fii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BlipProcessor, BlipForConditionalGeneration\n","from transformers import AdamW, get_cosine_schedule_with_warmup\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","from peft import get_peft_model, LoraConfig"],"metadata":{"id":"1Ri3oKgMHXSH","executionInfo":{"status":"ok","timestamp":1723635966503,"user_tz":-540,"elapsed":2502,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## **2-1. model 준비**"],"metadata":{"id":"zVF5dO33JTrY"}},{"cell_type":"code","source":["### Pre-trained model 불러오기\n","\n","# 전처리기\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\", do_rescale=False)\n","\n","# 모델\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"],"metadata":{"id":"aMYnu1OJHV_y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723635980055,"user_tz":-540,"elapsed":13553,"user":{"displayName":"차수빈","userId":"16979887891359732529"}},"outputId":"0a119f43-b4a0-46e1-ccae-ea9d0c0dd0b3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["### 장치 설정\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"FDfEYe8JHyiS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723635981118,"user_tz":-540,"elapsed":1065,"user":{"displayName":"차수빈","userId":"16979887891359732529"}},"outputId":"652ce39d-3c5c-4b13-acc4-657a5b0caa44"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BlipForConditionalGeneration(\n","  (vision_model): BlipVisionModel(\n","    (embeddings): BlipVisionEmbeddings(\n","      (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n","    )\n","    (encoder): BlipEncoder(\n","      (layers): ModuleList(\n","        (0-23): 24 x BlipEncoderLayer(\n","          (self_attn): BlipAttention(\n","            (dropout): Dropout(p=0.0, inplace=False)\n","            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n","            (projection): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (mlp): BlipMLP(\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (text_decoder): BlipTextLMHeadModel(\n","    (bert): BlipTextModel(\n","      (embeddings): BlipTextEmbeddings(\n","        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): BlipTextEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BlipTextLayer(\n","            (attention): BlipTextAttention(\n","              (self): BlipTextSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): BlipTextSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (crossattention): BlipTextAttention(\n","              (self): BlipTextSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=1024, out_features=768, bias=True)\n","                (value): Linear(in_features=1024, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): BlipTextSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): BlipTextIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BlipTextOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (cls): BlipTextOnlyMLMHead(\n","      (predictions): BlipTextLMPredictionHead(\n","        (transform): BlipTextPredictionHeadTransform(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (transform_act_fn): GELUActivation()\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["### target layer 가져오기\n","# 특정한 pattern으로 탐색\n","\n","def get_target_modules(model, patterns):\n","    target_modules = []\n","    for name, module in model.named_modules():\n","        for pattern in patterns:\n","            if pattern in name:\n","                target_modules.append(name)\n","    return target_modules"],"metadata":{"id":"McZ9yBvpdB0L","executionInfo":{"status":"ok","timestamp":1723635981118,"user_tz":-540,"elapsed":10,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["## 예시\n","# 조합에 따라 바꿔야함(layer 찍어서 구조 확인해보기)\n","\n","decoder_patterns = [\"self.query\", \"self.key\", \"self.value\", \"intermediate.dense\", \"output.dense\"]\n","encoder_patterns = [\"self_attn.qkv\", \"mlp.fc\"]\n","\n","encoder_modules = get_target_modules(model, encoder_patterns)[-18:] # 마지막 6개 layer만 가져오려고..\n","                                                                    # 조건에 따라 바꿔야 함\n","decoder_modules = get_target_modules(model, decoder_patterns)\n","\n","target_modules = encoder_modules+ decoder_modules"],"metadata":{"id":"-DKw4GNVdCZw","executionInfo":{"status":"ok","timestamp":1723635981118,"user_tz":-540,"elapsed":9,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["## PEFT + LoRA 설정\n","\n","## LoRA Configuration\n","lora_config = LoraConfig(\n","    r = 16, # LoRA의 rank\n","    lora_alpha = 32, # LoRA의 alpha\n","    lora_dropout = 0.05, # LoRA의 dropout 비율\n","    target_modules = target_modules,\n","    bias = \"none\"\n",")\n","\n","# LoRA 적용 모델\n","peft_model = get_peft_model(model, lora_config)"],"metadata":{"id":"8b_Vbwm5BrnA","executionInfo":{"status":"ok","timestamp":1723635981119,"user_tz":-540,"elapsed":10,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["for modules in peft_model.targeted_module_names:\n","  print(modules)"],"metadata":{"id":"a3tITECNnaCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723635981119,"user_tz":-540,"elapsed":10,"user":{"displayName":"차수빈","userId":"16979887891359732529"}},"outputId":"b5f7dcb4-1c5d-4441-cf04-2c958dd6eca1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["vision_model.encoder.layers.18.self_attn.qkv\n","vision_model.encoder.layers.18.mlp.fc1\n","vision_model.encoder.layers.18.mlp.fc2\n","vision_model.encoder.layers.19.self_attn.qkv\n","vision_model.encoder.layers.19.mlp.fc1\n","vision_model.encoder.layers.19.mlp.fc2\n","vision_model.encoder.layers.20.self_attn.qkv\n","vision_model.encoder.layers.20.mlp.fc1\n","vision_model.encoder.layers.20.mlp.fc2\n","vision_model.encoder.layers.21.self_attn.qkv\n","vision_model.encoder.layers.21.mlp.fc1\n","vision_model.encoder.layers.21.mlp.fc2\n","vision_model.encoder.layers.22.self_attn.qkv\n","vision_model.encoder.layers.22.mlp.fc1\n","vision_model.encoder.layers.22.mlp.fc2\n","vision_model.encoder.layers.23.self_attn.qkv\n","vision_model.encoder.layers.23.mlp.fc1\n","vision_model.encoder.layers.23.mlp.fc2\n","text_decoder.bert.encoder.layer.0.attention.self.query\n","text_decoder.bert.encoder.layer.0.attention.self.key\n","text_decoder.bert.encoder.layer.0.attention.self.value\n","text_decoder.bert.encoder.layer.0.attention.output.dense\n","text_decoder.bert.encoder.layer.0.crossattention.self.query\n","text_decoder.bert.encoder.layer.0.crossattention.self.key\n","text_decoder.bert.encoder.layer.0.crossattention.self.value\n","text_decoder.bert.encoder.layer.0.crossattention.output.dense\n","text_decoder.bert.encoder.layer.0.intermediate.dense\n","text_decoder.bert.encoder.layer.0.output.dense\n","text_decoder.bert.encoder.layer.1.attention.self.query\n","text_decoder.bert.encoder.layer.1.attention.self.key\n","text_decoder.bert.encoder.layer.1.attention.self.value\n","text_decoder.bert.encoder.layer.1.attention.output.dense\n","text_decoder.bert.encoder.layer.1.crossattention.self.query\n","text_decoder.bert.encoder.layer.1.crossattention.self.key\n","text_decoder.bert.encoder.layer.1.crossattention.self.value\n","text_decoder.bert.encoder.layer.1.crossattention.output.dense\n","text_decoder.bert.encoder.layer.1.intermediate.dense\n","text_decoder.bert.encoder.layer.1.output.dense\n","text_decoder.bert.encoder.layer.2.attention.self.query\n","text_decoder.bert.encoder.layer.2.attention.self.key\n","text_decoder.bert.encoder.layer.2.attention.self.value\n","text_decoder.bert.encoder.layer.2.attention.output.dense\n","text_decoder.bert.encoder.layer.2.crossattention.self.query\n","text_decoder.bert.encoder.layer.2.crossattention.self.key\n","text_decoder.bert.encoder.layer.2.crossattention.self.value\n","text_decoder.bert.encoder.layer.2.crossattention.output.dense\n","text_decoder.bert.encoder.layer.2.intermediate.dense\n","text_decoder.bert.encoder.layer.2.output.dense\n","text_decoder.bert.encoder.layer.3.attention.self.query\n","text_decoder.bert.encoder.layer.3.attention.self.key\n","text_decoder.bert.encoder.layer.3.attention.self.value\n","text_decoder.bert.encoder.layer.3.attention.output.dense\n","text_decoder.bert.encoder.layer.3.crossattention.self.query\n","text_decoder.bert.encoder.layer.3.crossattention.self.key\n","text_decoder.bert.encoder.layer.3.crossattention.self.value\n","text_decoder.bert.encoder.layer.3.crossattention.output.dense\n","text_decoder.bert.encoder.layer.3.intermediate.dense\n","text_decoder.bert.encoder.layer.3.output.dense\n","text_decoder.bert.encoder.layer.4.attention.self.query\n","text_decoder.bert.encoder.layer.4.attention.self.key\n","text_decoder.bert.encoder.layer.4.attention.self.value\n","text_decoder.bert.encoder.layer.4.attention.output.dense\n","text_decoder.bert.encoder.layer.4.crossattention.self.query\n","text_decoder.bert.encoder.layer.4.crossattention.self.key\n","text_decoder.bert.encoder.layer.4.crossattention.self.value\n","text_decoder.bert.encoder.layer.4.crossattention.output.dense\n","text_decoder.bert.encoder.layer.4.intermediate.dense\n","text_decoder.bert.encoder.layer.4.output.dense\n","text_decoder.bert.encoder.layer.5.attention.self.query\n","text_decoder.bert.encoder.layer.5.attention.self.key\n","text_decoder.bert.encoder.layer.5.attention.self.value\n","text_decoder.bert.encoder.layer.5.attention.output.dense\n","text_decoder.bert.encoder.layer.5.crossattention.self.query\n","text_decoder.bert.encoder.layer.5.crossattention.self.key\n","text_decoder.bert.encoder.layer.5.crossattention.self.value\n","text_decoder.bert.encoder.layer.5.crossattention.output.dense\n","text_decoder.bert.encoder.layer.5.intermediate.dense\n","text_decoder.bert.encoder.layer.5.output.dense\n","text_decoder.bert.encoder.layer.6.attention.self.query\n","text_decoder.bert.encoder.layer.6.attention.self.key\n","text_decoder.bert.encoder.layer.6.attention.self.value\n","text_decoder.bert.encoder.layer.6.attention.output.dense\n","text_decoder.bert.encoder.layer.6.crossattention.self.query\n","text_decoder.bert.encoder.layer.6.crossattention.self.key\n","text_decoder.bert.encoder.layer.6.crossattention.self.value\n","text_decoder.bert.encoder.layer.6.crossattention.output.dense\n","text_decoder.bert.encoder.layer.6.intermediate.dense\n","text_decoder.bert.encoder.layer.6.output.dense\n","text_decoder.bert.encoder.layer.7.attention.self.query\n","text_decoder.bert.encoder.layer.7.attention.self.key\n","text_decoder.bert.encoder.layer.7.attention.self.value\n","text_decoder.bert.encoder.layer.7.attention.output.dense\n","text_decoder.bert.encoder.layer.7.crossattention.self.query\n","text_decoder.bert.encoder.layer.7.crossattention.self.key\n","text_decoder.bert.encoder.layer.7.crossattention.self.value\n","text_decoder.bert.encoder.layer.7.crossattention.output.dense\n","text_decoder.bert.encoder.layer.7.intermediate.dense\n","text_decoder.bert.encoder.layer.7.output.dense\n","text_decoder.bert.encoder.layer.8.attention.self.query\n","text_decoder.bert.encoder.layer.8.attention.self.key\n","text_decoder.bert.encoder.layer.8.attention.self.value\n","text_decoder.bert.encoder.layer.8.attention.output.dense\n","text_decoder.bert.encoder.layer.8.crossattention.self.query\n","text_decoder.bert.encoder.layer.8.crossattention.self.key\n","text_decoder.bert.encoder.layer.8.crossattention.self.value\n","text_decoder.bert.encoder.layer.8.crossattention.output.dense\n","text_decoder.bert.encoder.layer.8.intermediate.dense\n","text_decoder.bert.encoder.layer.8.output.dense\n","text_decoder.bert.encoder.layer.9.attention.self.query\n","text_decoder.bert.encoder.layer.9.attention.self.key\n","text_decoder.bert.encoder.layer.9.attention.self.value\n","text_decoder.bert.encoder.layer.9.attention.output.dense\n","text_decoder.bert.encoder.layer.9.crossattention.self.query\n","text_decoder.bert.encoder.layer.9.crossattention.self.key\n","text_decoder.bert.encoder.layer.9.crossattention.self.value\n","text_decoder.bert.encoder.layer.9.crossattention.output.dense\n","text_decoder.bert.encoder.layer.9.intermediate.dense\n","text_decoder.bert.encoder.layer.9.output.dense\n","text_decoder.bert.encoder.layer.10.attention.self.query\n","text_decoder.bert.encoder.layer.10.attention.self.key\n","text_decoder.bert.encoder.layer.10.attention.self.value\n","text_decoder.bert.encoder.layer.10.attention.output.dense\n","text_decoder.bert.encoder.layer.10.crossattention.self.query\n","text_decoder.bert.encoder.layer.10.crossattention.self.key\n","text_decoder.bert.encoder.layer.10.crossattention.self.value\n","text_decoder.bert.encoder.layer.10.crossattention.output.dense\n","text_decoder.bert.encoder.layer.10.intermediate.dense\n","text_decoder.bert.encoder.layer.10.output.dense\n","text_decoder.bert.encoder.layer.11.attention.self.query\n","text_decoder.bert.encoder.layer.11.attention.self.key\n","text_decoder.bert.encoder.layer.11.attention.self.value\n","text_decoder.bert.encoder.layer.11.attention.output.dense\n","text_decoder.bert.encoder.layer.11.crossattention.self.query\n","text_decoder.bert.encoder.layer.11.crossattention.self.key\n","text_decoder.bert.encoder.layer.11.crossattention.self.value\n","text_decoder.bert.encoder.layer.11.crossattention.output.dense\n","text_decoder.bert.encoder.layer.11.intermediate.dense\n","text_decoder.bert.encoder.layer.11.output.dense\n"]}]},{"cell_type":"markdown","source":["## **2-2. 평가 지표 준비**"],"metadata":{"id":"orw0RU_Iv3pa"}},{"cell_type":"code","source":["!git clone https://github.com/salaniz/pycocoevalcap\n","!pip install git+https://github.com/salaniz/pycocoevalcap.git"],"metadata":{"id":"p0uZq4bcv7SW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pycocoevalcap.bleu.bleu import Bleu\n","from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.cider.cider import Cider\n","from pycocoevalcap.spice.spice import Spice"],"metadata":{"id":"4NGbGt1Hv6Q4","executionInfo":{"status":"ok","timestamp":1723635981119,"user_tz":-540,"elapsed":7,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["### 평가 지표 계산\n","\n","def compute_metrics(preds, labels):\n","  scorers = [\n","      (Bleu(4), \"BLEU-4\"),\n","      (Meteor(), \"METEOR\"),\n","      (Rouge(), \"ROUGE\"),\n","      (Cider(), \"CIDEr\"),\n","      (Spice(), \"SPICE\")\n","  ]\n","\n","  results = {}\n","  for scorer, method in scorers:\n","    score, _ = scorer.compute_score({i: [labels[i]] for i in range(len(labels))}, {i: [preds[i]] for i in range(len(preds))})\n","    results[method] = score if isinstance(score, float) else score[-1]\n","\n","  return results"],"metadata":{"id":"bI-e_04XwKYV","executionInfo":{"status":"ok","timestamp":1723635981119,"user_tz":-540,"elapsed":7,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## **2-3. 학습 및 검증**"],"metadata":{"id":"jBF99PykLJes"}},{"cell_type":"code","source":["### 옵티마이저 및 Learning Rate Schedule 정의\n","\n","## 옵티마이저\n","optimizer = AdamW(filter(lambda p: p.requires_grad, peft_model.parameters()),\n","                  lr = 2e-5, weight_decay = 0.05)\n","\n","## learning rate schedule\n","num_training_steps = len(train_loader) * 10  # 10 에포크\n","num_warmup_steps = int(0.1 * num_training_steps)  # Warmup 비율 10%\n","scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = num_warmup_steps,\n","                                            num_training_steps = num_training_steps)"],"metadata":{"id":"ByVWXJoIxOoF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723635981119,"user_tz":-540,"elapsed":6,"user":{"displayName":"차수빈","userId":"16979887891359732529"}},"outputId":"69b07b0d-dd75-4297-efd4-e06d428c7d65"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["## 학습 및 검증 손실, 평가 지표 기록\n","\n","train_losses = []\n","val_losses = []\n","best_val_loss = float('inf')\n","\n","bleu_scores = []\n","meteor_scores = []\n","rouge_scores = []\n","cider_scores = []\n","spice_scores = []"],"metadata":{"id":"abQpBO8rHpeF","executionInfo":{"status":"ok","timestamp":1723635981119,"user_tz":-540,"elapsed":4,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["### 학습을 위한 함수\n","\n","def train(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs):\n","  global best_val_loss\n","\n","  for epoch in range(num_epochs):\n","    print(f\"=== Epoch {epoch+1} ===\")\n","    print(\"-\" * 20)\n","\n","    ## Training loop\n","    model.train()  # 모델 학습 모드로 설정\n","    epoch_train_loss = 0\n","\n","    for batch_idx, (images, captions) in enumerate(tqdm(train_loader, desc = f\"Training Epoch {epoch+1}/{num_epochs}\")):\n","      images = images.to(device)\n","\n","      # 전처리\n","      inputs = processor(images = images, text = captions, return_tensors = \"pt\", padding = True).to(device)\n","\n","      # 모델에 입력\n","      outputs = model(**inputs, labels = inputs.input_ids)\n","\n","      # 손실 계산 및 역전파\n","      loss = outputs.loss\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","      scheduler.step()  # Learning rate 스케줄링\n","\n","      epoch_train_loss += loss.item()\n","\n","    avg_train_loss = epoch_train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n","\n","\n","    ## Validation loop\n","    model.eval()  # 모델 평가 모드로 설정\n","    epoch_val_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","      for images, captions in tqdm(val_loader, desc = \"Validation\"):\n","        images = images.to(device)\n","        inputs = processor(images = images, text = captions, return_tensors = \"pt\", padding = True).to(device)\n","\n","        outputs = model(**inputs, labels = inputs.input_ids)\n","        loss = outputs.loss\n","        epoch_val_loss += loss.item()\n","\n","        # Generate captions\n","        generated_ids = model.generate(pixel_values = inputs.pixel_values, max_length = 30)\n","        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens = True)\n","\n","        # Collect predictions and labels for evaluation\n","        all_preds.extend(generated_texts)\n","        all_labels.extend(captions)\n","\n","    avg_val_loss = epoch_val_loss / len(val_loader)\n","    val_losses.append(avg_val_loss)\n","    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n","\n","    ## Calculate evaluation metrics\n","    metrics = compute_metrics(all_preds, all_labels)\n","\n","    bleu_scores.append(metrics['BLEU-4'])\n","    meteor_scores.append(metrics['METEOR'])\n","    rouge_scores.append(metrics['ROUGE'])\n","    cider_scores.append(metrics['CIDEr'])\n","    spice_scores.append(metrics['SPICE'])\n","\n","    print(f\"BLEU: {metrics['BLEU-4']:.4f}, METEOR: {metrics['METEOR']:.4f}, ROUGE: {metrics['ROUGE']:.4f}, CIDEr: {metrics['CIDEr']:.4f}, SPICE: {metrics['SPICE']:.4f}\")\n","\n","\n","    ## Best model 저장\n","    if avg_val_loss < best_val_loss:\n","      best_val_loss = avg_val_loss\n","      torch.save(model.state_dict(), \"/content/drive/MyDrive/6th-project/final/model/model_trial_xxx.pth\") # 모델명 trial에 맞게 바꿔주세요.\n","      print(f\"Model saved at Epoch {epoch+1} with Validation Loss: {avg_val_loss:.4f}\")\n","\n","    ## Clear cache\n","    torch.cuda.empty_cache()"],"metadata":{"id":"XfWZsu_QLWRT","executionInfo":{"status":"ok","timestamp":1723635981939,"user_tz":-540,"elapsed":824,"user":{"displayName":"차수빈","userId":"16979887891359732529"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","\n","num_epochs = 10\n","train(peft_model, train_loader, val_loader, optimizer, scheduler, device, num_epochs)"],"metadata":{"id":"qoRQZCLDNips","colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"status":"error","timestamp":1723636339329,"user_tz":-540,"elapsed":277267,"user":{"displayName":"차수빈","userId":"16979887891359732529"}},"outputId":"80df6844-47d2-406d-80e8-8a2affbaf72f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Epoch 1 ===\n","--------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/10:  13%|█▎        | 79/625 [04:36<31:50,  3.50s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-a35e22123a07>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-fe75d2620af6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;31m# 손실 계산 및 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["- layer 풀어주는 개수에 따라서 사용되는 리소스가 달라지니, GPU는 알아서 선택적으로 활용하세요.\n","  - T4도 버티기는 하는데 조금 불안불안하네요,,\n","  - GPU 종류 선택 자체가 모델링 성능에 미치는 영향은 다른 조건들이 모두 동일하기에 매우 미미함"],"metadata":{"id":"ca0MfoO39UHG"}},{"cell_type":"markdown","source":["## **2-3. 결과 확인**"],"metadata":{"id":"k4oKDpcIPA7s"}},{"cell_type":"code","source":["### 시각화\n","epochs = range(1, num_epochs + 1)\n","\n","plt.figure(figsize = (15, 15))\n","\n","# 손실 곡선\n","plt.subplot(3, 1, 1)\n","plt.plot(epochs, train_losses, marker='o', label='Training Loss')\n","plt.plot(epochs, val_losses, marker='o', label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","\n","# 평가 지표 1 (BLEU, METEOR)\n","plt.subplot(3, 1, 2)\n","plt.plot(epochs, bleu_scores, marker='o', label='BLEU')\n","plt.plot(epochs, meteor_scores, marker='o', label='METEOR')\n","plt.xlabel('Epoch')\n","plt.ylabel('Score')\n","plt.title('BLEU and METEOR Scores')\n","plt.legend()\n","\n","# 평가 지표 2 (ROUGE, CIDEr, SPICE)\n","plt.subplot(3, 1, 3)\n","plt.plot(epochs, rouge_scores, marker='o', label='ROUGE')\n","plt.plot(epochs, cider_scores, marker='o', label='CIDEr')\n","plt.plot(epochs, spice_scores, marker='o', label='SPICE')\n","plt.xlabel('Epoch')\n","plt.ylabel('Score')\n","plt.title('ROUGE, CIDEr and SPICE Scores')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"w3YY4C7vNnGv"},"execution_count":null,"outputs":[]}]}